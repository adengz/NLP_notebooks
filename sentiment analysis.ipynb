{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/b5/c7a92c7ce5d4b353b70b4b5b4385687206c8b230ddfe08746ab0fd310a3a/spacy-2.3.2-cp36-cp36m-manylinux1_x86_64.whl (9.9MB)\n",
      "\u001b[K     |████████████████████████████████| 10.0MB 13.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.0)\n",
      "Collecting plac<1.2.0,>=0.9.6 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/25/8cb4d3dfce05cd261adadfec0cbc4d90a7ddf6abac888b2d354042986b64/cymem-2.0.4.tar.gz (56kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 2.5MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting blis<0.5.0,>=0.4.0 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/19/f95c75562d18eb27219df3a3590b911e78d131b68466ad79fdf5847eaac4/blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 53.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/e3/c1c55784a4de82ece935e515670c9da86f7766a3d4d680843ef87553b57a/murmurhash-1.0.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/80/9587ed3fe0d6e00e039d89a3998ddfbe0560c27faddf4a22a4cd782aecdb/preshed-3.0.4-cp36-cp36m-manylinux1_x86_64.whl (284kB)\n",
      "\u001b[K     |████████████████████████████████| 286kB 53.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc==7.4.1 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/ae/ef3ae5e93639c0ef8e3eb32e3c18341e511b3c515fcfc603f4b808087651/thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 44.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.51.0)\n",
      "Collecting wasabi<1.1.0,>=0.4.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/10/55f3cf6b52cc89107b3e1b88fcf39719392b377a3d78ca61da85934d0d10/wasabi-0.8.0-py3-none-any.whl\n",
      "Collecting srsly<1.1.0,>=1.0.2 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/61/5a503487f711f42136a3a0986ac1bb46b2c96134634332238cc003e78a05/srsly-1.0.4.tar.gz (193kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 38.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (41.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.25.0)\n",
      "Collecting importlib-metadata>=0.20; python_version < \"3.8\" (from catalogue<1.1.0,>=0.0.7->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/6d/f4bb28424bc677bce1210bc19f69a43efe823e294325606ead595211f93e/importlib_metadata-2.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.11.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.2)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/41/ad/6a4f1a124b325618a7fb758b885b68ff7b058eec47d9220a12ab38d90b1f/zipp-3.4.0-py3-none-any.whl\n",
      "Building wheels for collected packages: cymem, srsly\n",
      "  Building wheel for cymem (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cymem: filename=cymem-2.0.4-cp36-cp36m-linux_x86_64.whl size=122183 sha256=ca25412deb55811b7bdcb3c04a934f3b4271659ee5ca436ea1376f420c8bfa5c\n",
      "  Stored in directory: /root/.cache/pip/wheels/f3/41/71/a2767add74e8f0e87a56d1c5efc498a4693546721b3fedfbd1\n",
      "  Building wheel for srsly (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for srsly: filename=srsly-1.0.4-cp36-cp36m-linux_x86_64.whl size=573606 sha256=4a2a41bd8593dd80161d201fb886e8cf93a16afd09aa30dfda2ec258bdd46591\n",
      "  Stored in directory: /root/.cache/pip/wheels/39/68/dd/b992bd709c57542d14c72358d46e6a45b211820010d2e808d2\n",
      "Successfully built cymem srsly\n",
      "Installing collected packages: plac, cymem, blis, zipp, importlib-metadata, catalogue, murmurhash, preshed, wasabi, srsly, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.4 importlib-metadata-2.0.0 murmurhash-1.0.4 plac-1.1.3 preshed-3.0.4 spacy-2.3.2 srsly-1.0.4 thinc-7.4.1 wasabi-0.8.0 zipp-3.4.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0MB)\n",
      "\u001b[K     |████████████████████████████████| 12.1MB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.17.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.51.0)\n",
      "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (41.0.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.11.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-cp36-none-any.whl size=12047109 sha256=2427418803419127da02b680ad7a9e376f5191d971feb4cf61046c36149805e5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3zz1ia0d/wheels/2b/3f/41/f0b92863355c3ba34bb32b37d8a0c662959da0058202094f46\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy link en_core_web_sm en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load IMDb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "aclImdb_v1.tar.gz:   5%|▍         | 3.98M/84.1M [00:00<00:02, 39.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:02<00:00, 33.4MB/s]\n",
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "TEXT = Field(tokenize='spacy')\n",
    "LABEL = LabelField(dtype=torch.float)\n",
    "\n",
    "train_data, test_data = IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Zentropa', 'has', 'much', 'in', 'common', 'with', 'The', 'Third', 'Man', ',', 'another', 'noir', '-', 'like', 'film', 'set', 'among', 'the', 'rubble', 'of', 'postwar', 'Europe', '.', 'Like', 'TTM', ',', 'there', 'is', 'much', 'inventive', 'camera', 'work', '.', 'There', 'is', 'an', 'innocent', 'American', 'who', 'gets', 'emotionally', 'involved', 'with', 'a', 'woman', 'he', 'does', \"n't\", 'really', 'understand', ',', 'and', 'whose', 'naivety', 'is', 'all', 'the', 'more', 'striking', 'in', 'contrast', 'with', 'the', 'natives.<br', '/><br', '/>But', 'I', \"'d\", 'have', 'to', 'say', 'that', 'The', 'Third', 'Man', 'has', 'a', 'more', 'well', '-', 'crafted', 'storyline', '.', 'Zentropa', 'is', 'a', 'bit', 'disjointed', 'in', 'this', 'respect', '.', 'Perhaps', 'this', 'is', 'intentional', ':', 'it', 'is', 'presented', 'as', 'a', 'dream', '/', 'nightmare', ',', 'and', 'making', 'it', 'too', 'coherent', 'would', 'spoil', 'the', 'effect', '.', '<', 'br', '/><br', '/>This', 'movie', 'is', 'unrelentingly', 'grim--\"noir', '\"', 'in', 'more', 'than', 'one', 'sense', ';', 'one', 'never', 'sees', 'the', 'sun', 'shine', '.', 'Grim', ',', 'but', 'intriguing', ',', 'and', 'frightening', '.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split validation set out of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                            \n",
      "100%|█████████▉| 399999/400000 [00:22<00:00, 17779.48it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, max_size=25000, vectors='glove.6B.100d')\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}')\n",
    "print(f'Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neg', 0), ('pos', 1)]\n",
      "[('<unk>', 0), ('<pad>', 1), ('the', 2), (',', 3), ('.', 4), ('a', 5), ('and', 6), ('of', 7), ('to', 8), ('is', 9), ('in', 10), ('I', 11), ('it', 12), ('that', 13), ('\"', 14), (\"'s\", 15), ('this', 16), ('-', 17), ('/><br', 18), ('was', 19)]\n",
      "\n",
      "[('the', 204107), (',', 194270), ('.', 166464), ('a', 110139), ('and', 109993), ('of', 101553), ('to', 94337), ('is', 76827), ('in', 61595), ('I', 54376), ('it', 53662), ('that', 49415), ('\"', 44401), (\"'s\", 43781), ('this', 42212), ('-', 37510), ('/><br', 35656), ('was', 35057), ('as', 30614), ('with', 30210)]\n"
     ]
    }
   ],
   "source": [
    "print(list(LABEL.vocab.stoi.items()))\n",
    "print(list(TEXT.vocab.stoi.items())[:20])\n",
    "print()\n",
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([1134, 64])\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_iter)).label.shape)\n",
    "print(next(iter(train_iter)).text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1103, 64])\n",
      "['This', 'film', 'was', 'a', 'waste', 'of', 'time', ',', 'even', 'rented', 'on', 'DVD', '.', 'If', 'super', '-', '<unk>', 'camera', 'shots', 'get', 'any', 'faster', 'than', 'this', ',', 'we', 'might', 'as', 'well', 'pay', 'twenty', 'bucks', 'to', 'get', 'in', 'the', '<unk>', ',', 'get', 'popcorn', ',', 'and', 'watch', 'the', '<unk>', 'spin', '.', 'Jet', 'Li', 'is', 'so', 'much', 'better', 'than', 'this', '.', 'One', 'can', 'only', 'hope', 'that', 'he', 'wo', \"n't\", 'be', 'making', 'deals', 'anytime', 'soon', 'to', 'make', 'another', 'cliche', '-', 'ridden', 'film', 'like', 'The', '<unk>', '/><br', '/>If', 'there', \"'s\", 'one', 'film', 'you', 'should', 'avoid', ',', 'this', 'is', '\"', 'The', 'One', '\"', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(batch.text.shape) \n",
    "print([TEXT.vocab.itos[i] for i in batch.text[:, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some utility functions for training/evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "EMBEDDING_DIM = TEXT.vocab.vectors.shape[1]\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def init_params(model):\n",
    "    model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "    for idx in [UNK_IDX, PAD_IDX]:\n",
    "        model.embedding.weight.data[idx] = torch.zeros(EMBEDDING_DIM)\n",
    "        \n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y)\n",
    "    acc = correct.sum() / len(y)\n",
    "    return acc.item()\n",
    "\n",
    "def train_epoch(model, optimizer, iterator=train_iter, criterion=loss_fn):\n",
    "    epoch_loss = epoch_acc = sample_count = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, y = batch.text, batch.label\n",
    "        output = model(x).squeeze()\n",
    "        loss = criterion(output, y)\n",
    "        acc = binary_accuracy(output, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size = len(y)\n",
    "        sample_count += batch_size\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        epoch_acc += acc * batch_size\n",
    "        \n",
    "    return epoch_loss / sample_count, epoch_acc / sample_count\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, iterator, criterion=loss_fn):\n",
    "    epoch_loss = epoch_acc = sample_count = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        x, y = batch.text, batch.label\n",
    "        output = model(x).squeeze()\n",
    "        loss = criterion(output, y)\n",
    "        acc = binary_accuracy(output, y)\n",
    "        \n",
    "        batch_size = len(y)\n",
    "        sample_count += batch_size\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        epoch_acc += acc * batch_size\n",
    "        \n",
    "    return epoch_loss / sample_count, epoch_acc / sample_count\n",
    "\n",
    "def epoch_time(start, end):\n",
    "    elapsed = end - start\n",
    "    mins = int(elapsed / 60)\n",
    "    secs = int(elapsed - mins * 60)\n",
    "    return mins, secs\n",
    "\n",
    "def train(model, filename, optim=Adam, epochs=20):\n",
    "    optimizer = optim(model.parameters())\n",
    "    min_valid_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, optimizer)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iter)\n",
    "        \n",
    "        end = time.time()\n",
    "        mins, secs = epoch_time(start, end)\n",
    "        \n",
    "        if valid_loss < min_valid_loss:\n",
    "            min_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f'{filename}.pt')\n",
    "            \n",
    "        print(f'Epoch: {epoch + 1:02} | Epoch Time: {mins}m {secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word averaging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordAVGModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size=len(TEXT.vocab), embedding_dim=EMBEDDING_DIM, output_dim=1, pad_idx=PAD_IDX):\n",
    "        super(WordAVGModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "    \n",
    "    def forward(self, inp): # seq_len, batch_size\n",
    "        embedded = self.embedding(inp) # seq_len, batch_size, embedding_dim\n",
    "        pooled = embedded.mean(0) # batch_size, embedding_dim\n",
    "        return self.fc(pooled) # batch_size, output_dim\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500301"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_avg_model = WordAVGModel()\n",
    "count_params(word_avg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.685 | Train Acc: 60.22%\n",
      "\t Val. Loss: 0.625 |  Val. Acc: 68.57%\n",
      "Epoch: 02 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.641 | Train Acc: 74.00%\n",
      "\t Val. Loss: 0.521 |  Val. Acc: 75.20%\n",
      "Epoch: 03 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.568 | Train Acc: 79.13%\n",
      "\t Val. Loss: 0.457 |  Val. Acc: 80.40%\n",
      "Epoch: 04 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.495 | Train Acc: 83.37%\n",
      "\t Val. Loss: 0.443 |  Val. Acc: 82.96%\n",
      "Epoch: 05 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.433 | Train Acc: 86.12%\n",
      "\t Val. Loss: 0.435 |  Val. Acc: 84.88%\n",
      "Epoch: 06 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.382 | Train Acc: 88.01%\n",
      "\t Val. Loss: 0.449 |  Val. Acc: 86.00%\n",
      "Epoch: 07 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.342 | Train Acc: 89.40%\n",
      "\t Val. Loss: 0.476 |  Val. Acc: 86.76%\n",
      "Epoch: 08 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.310 | Train Acc: 90.34%\n",
      "\t Val. Loss: 0.494 |  Val. Acc: 87.21%\n",
      "Epoch: 09 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.286 | Train Acc: 91.07%\n",
      "\t Val. Loss: 0.519 |  Val. Acc: 87.47%\n",
      "Epoch: 10 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.263 | Train Acc: 91.71%\n",
      "\t Val. Loss: 0.540 |  Val. Acc: 87.87%\n",
      "Epoch: 11 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.245 | Train Acc: 92.39%\n",
      "\t Val. Loss: 0.563 |  Val. Acc: 88.05%\n",
      "Epoch: 12 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.229 | Train Acc: 92.86%\n",
      "\t Val. Loss: 0.583 |  Val. Acc: 88.25%\n",
      "Epoch: 13 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.214 | Train Acc: 93.46%\n",
      "\t Val. Loss: 0.601 |  Val. Acc: 88.40%\n",
      "Epoch: 14 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.201 | Train Acc: 93.76%\n",
      "\t Val. Loss: 0.622 |  Val. Acc: 88.55%\n",
      "Epoch: 15 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.190 | Train Acc: 94.26%\n",
      "\t Val. Loss: 0.642 |  Val. Acc: 88.68%\n",
      "Epoch: 16 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.177 | Train Acc: 94.53%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 88.83%\n",
      "Epoch: 17 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.168 | Train Acc: 94.94%\n",
      "\t Val. Loss: 0.681 |  Val. Acc: 88.88%\n",
      "Epoch: 18 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.158 | Train Acc: 95.16%\n",
      "\t Val. Loss: 0.699 |  Val. Acc: 88.87%\n",
      "Epoch: 19 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.150 | Train Acc: 95.57%\n",
      "\t Val. Loss: 0.718 |  Val. Acc: 88.92%\n",
      "Epoch: 20 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.141 | Train Acc: 95.86%\n",
      "\t Val. Loss: 0.736 |  Val. Acc: 89.09%\n"
     ]
    }
   ],
   "source": [
    "word_avg_model.to(device)\n",
    "init_params(word_avg_model)\n",
    "train(word_avg_model, 'word_avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_layers, bidirectional, dropout, \n",
    "                 vocab_size=len(TEXT.vocab), embedding_dim=EMBEDDING_DIM, \n",
    "                 output_dim=1, pad_idx=PAD_IDX):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.fc = nn.Linear(self.num_directions * hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, inp): # seq_len, batch_size\n",
    "        embedded = self.dropout(self.embedding(inp)) # seq_len, batch_size, embedding_dim\n",
    "        _, (hidden, _) = self.rnn(embedded) # num_layers * num_directions, batch_size, hidden_dim\n",
    "        last_hidden = hidden[-self.num_directions:, :, :] # num_directions, batch_size, hidden_dim\n",
    "        if self.num_directions == 1:\n",
    "            output = last_hidden.squeeze(0)\n",
    "        else:\n",
    "            output = torch.cat([last_hidden[0, :, :], last_hidden[1, :, :]], dim=1)\n",
    "        output = self.dropout(output) # batch_size, hidden_dim * num_directions\n",
    "        return self.fc(output.squeeze(0)) # (batch_size > 1), output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4810857"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT=0.5\n",
    "\n",
    "rnn_model = RNNModel(HIDDEN_DIM, NUM_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "count_params(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 5s\n",
      "\tTrain Loss: 0.664 | Train Acc: 59.21%\n",
      "\t Val. Loss: 0.558 |  Val. Acc: 73.53%\n",
      "Epoch: 02 | Epoch Time: 2m 4s\n",
      "\tTrain Loss: 0.599 | Train Acc: 67.99%\n",
      "\t Val. Loss: 0.435 |  Val. Acc: 81.12%\n",
      "Epoch: 03 | Epoch Time: 2m 4s\n",
      "\tTrain Loss: 0.442 | Train Acc: 80.62%\n",
      "\t Val. Loss: 0.438 |  Val. Acc: 80.84%\n",
      "Epoch: 04 | Epoch Time: 2m 5s\n",
      "\tTrain Loss: 0.300 | Train Acc: 88.25%\n",
      "\t Val. Loss: 0.316 |  Val. Acc: 87.23%\n",
      "Epoch: 05 | Epoch Time: 2m 3s\n",
      "\tTrain Loss: 0.219 | Train Acc: 91.57%\n",
      "\t Val. Loss: 0.287 |  Val. Acc: 88.80%\n",
      "Epoch: 06 | Epoch Time: 2m 5s\n",
      "\tTrain Loss: 0.159 | Train Acc: 94.19%\n",
      "\t Val. Loss: 0.295 |  Val. Acc: 88.01%\n",
      "Epoch: 07 | Epoch Time: 2m 4s\n",
      "\tTrain Loss: 0.130 | Train Acc: 95.26%\n",
      "\t Val. Loss: 0.357 |  Val. Acc: 88.57%\n",
      "Epoch: 08 | Epoch Time: 2m 4s\n",
      "\tTrain Loss: 0.097 | Train Acc: 96.74%\n",
      "\t Val. Loss: 0.337 |  Val. Acc: 88.83%\n",
      "Epoch: 09 | Epoch Time: 2m 4s\n",
      "\tTrain Loss: 0.075 | Train Acc: 97.45%\n",
      "\t Val. Loss: 0.390 |  Val. Acc: 89.04%\n",
      "Epoch: 10 | Epoch Time: 2m 3s\n",
      "\tTrain Loss: 0.062 | Train Acc: 98.01%\n",
      "\t Val. Loss: 0.397 |  Val. Acc: 88.99%\n",
      "Epoch: 11 | Epoch Time: 2m 5s\n",
      "\tTrain Loss: 0.048 | Train Acc: 98.46%\n",
      "\t Val. Loss: 0.408 |  Val. Acc: 88.37%\n",
      "Epoch: 12 | Epoch Time: 2m 4s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.73%\n",
      "\t Val. Loss: 0.430 |  Val. Acc: 87.88%\n",
      "Epoch: 13 | Epoch Time: 2m 5s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.74%\n",
      "\t Val. Loss: 0.460 |  Val. Acc: 88.49%\n",
      "Epoch: 14 | Epoch Time: 2m 4s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.82%\n",
      "\t Val. Loss: 0.461 |  Val. Acc: 87.92%\n",
      "Epoch: 15 | Epoch Time: 2m 5s\n",
      "\tTrain Loss: 0.026 | Train Acc: 99.18%\n",
      "\t Val. Loss: 0.486 |  Val. Acc: 87.71%\n",
      "Epoch: 16 | Epoch Time: 2m 5s\n",
      "\tTrain Loss: 0.024 | Train Acc: 99.22%\n",
      "\t Val. Loss: 0.496 |  Val. Acc: 87.71%\n",
      "Epoch: 17 | Epoch Time: 2m 5s\n",
      "\tTrain Loss: 0.019 | Train Acc: 99.41%\n",
      "\t Val. Loss: 0.584 |  Val. Acc: 88.29%\n",
      "Epoch: 18 | Epoch Time: 2m 3s\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.53%\n",
      "\t Val. Loss: 0.647 |  Val. Acc: 88.73%\n",
      "Epoch: 19 | Epoch Time: 2m 4s\n",
      "\tTrain Loss: 0.017 | Train Acc: 99.47%\n",
      "\t Val. Loss: 0.541 |  Val. Acc: 87.75%\n",
      "Epoch: 20 | Epoch Time: 2m 5s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.50%\n",
      "\t Val. Loss: 0.701 |  Val. Acc: 88.05%\n"
     ]
    }
   ],
   "source": [
    "rnn_model.to(device)\n",
    "init_params(rnn_model)\n",
    "train(rnn_model, 'rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_filters, filter_sizes, dropout, \n",
    "                 vocab_size=len(TEXT.vocab), embedding_dim=EMBEDDING_DIM, \n",
    "                 output_dim=1, pad_idx=PAD_IDX):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, n_filters, kernel_size=(fs, embedding_dim)) for fs in filter_sizes])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "    def forward(self, inp): # seq_len, batch_size\n",
    "        inp = inp.permute(1, 0) # batch_size, seq_len\n",
    "        embedded = self.dropout(self.embedding(inp)) # batch_size, seq_len, embedding_dim\n",
    "        embedded = embedded.unsqueeze(1) # batch_size, 1, seq_len, embedding_dim\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs] # [batch_size, n_filters, seq_len - filter_sizes + 1] * len(filter_sizes)\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved] # [batch_size, n_filters] * len(filter_sizes)\n",
    "        concated = self.dropout(torch.cat(pooled, dim=1)) # batch_size, n_filters * len(filter_sizes)\n",
    "        return self.fc(concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2620801"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "DROPOUT = 0.5\n",
    "\n",
    "cnn_model = CNNModel(N_FILTERS, FILTER_SIZES, DROPOUT)\n",
    "count_params(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.620 | Train Acc: 64.51%\n",
      "\t Val. Loss: 0.422 |  Val. Acc: 81.53%\n",
      "Epoch: 02 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.419 | Train Acc: 80.89%\n",
      "\t Val. Loss: 0.354 |  Val. Acc: 84.99%\n",
      "Epoch: 03 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.337 | Train Acc: 85.24%\n",
      "\t Val. Loss: 0.327 |  Val. Acc: 86.47%\n",
      "Epoch: 04 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.294 | Train Acc: 87.55%\n",
      "\t Val. Loss: 0.304 |  Val. Acc: 87.64%\n",
      "Epoch: 05 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.249 | Train Acc: 89.66%\n",
      "\t Val. Loss: 0.287 |  Val. Acc: 88.39%\n",
      "Epoch: 06 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.220 | Train Acc: 91.17%\n",
      "\t Val. Loss: 0.284 |  Val. Acc: 88.68%\n",
      "Epoch: 07 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.177 | Train Acc: 93.10%\n",
      "\t Val. Loss: 0.290 |  Val. Acc: 88.60%\n",
      "Epoch: 08 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.162 | Train Acc: 93.77%\n",
      "\t Val. Loss: 0.293 |  Val. Acc: 88.76%\n",
      "Epoch: 09 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.137 | Train Acc: 94.73%\n",
      "\t Val. Loss: 0.319 |  Val. Acc: 87.96%\n",
      "Epoch: 10 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.117 | Train Acc: 95.79%\n",
      "\t Val. Loss: 0.316 |  Val. Acc: 88.28%\n",
      "Epoch: 11 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.101 | Train Acc: 96.17%\n",
      "\t Val. Loss: 0.333 |  Val. Acc: 88.15%\n",
      "Epoch: 12 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.091 | Train Acc: 96.66%\n",
      "\t Val. Loss: 0.349 |  Val. Acc: 88.16%\n",
      "Epoch: 13 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.080 | Train Acc: 96.94%\n",
      "\t Val. Loss: 0.358 |  Val. Acc: 88.16%\n",
      "Epoch: 14 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.070 | Train Acc: 97.45%\n",
      "\t Val. Loss: 0.377 |  Val. Acc: 87.88%\n",
      "Epoch: 15 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.065 | Train Acc: 97.65%\n",
      "\t Val. Loss: 0.394 |  Val. Acc: 88.05%\n",
      "Epoch: 16 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.056 | Train Acc: 97.97%\n",
      "\t Val. Loss: 0.418 |  Val. Acc: 87.85%\n",
      "Epoch: 17 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.057 | Train Acc: 98.03%\n",
      "\t Val. Loss: 0.424 |  Val. Acc: 87.53%\n",
      "Epoch: 18 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.048 | Train Acc: 98.30%\n",
      "\t Val. Loss: 0.443 |  Val. Acc: 87.69%\n",
      "Epoch: 19 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.72%\n",
      "\t Val. Loss: 0.469 |  Val. Acc: 87.55%\n",
      "Epoch: 20 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.53%\n",
      "\t Val. Loss: 0.474 |  Val. Acc: 87.53%\n"
     ]
    }
   ],
   "source": [
    "cnn_model.to(device)\n",
    "init_params(cnn_model)\n",
    "train(cnn_model, 'cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the best model parameters for each model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_avg_model.load_state_dict(torch.load('word_avg.pt'))\n",
    "rnn_model.load_state_dict(torch.load('rnn.pt'))\n",
    "cnn_model.load_state_dict(torch.load('cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_metrics(model):\n",
    "    loss, acc = evaluate(model, test_iter)\n",
    "    print(f'Test Loss: {loss:.3f} |  Test Acc: {acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordAVGModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.431 |  Test Acc: 84.34%\n",
      "\n",
      "RNNModel\n",
      "Test Loss: 0.331 |  Test Acc: 86.33%\n",
      "\n",
      "CNNModel\n",
      "Test Loss: 0.296 |  Test Acc: 87.86%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [word_avg_model, rnn_model, cnn_model]:\n",
    "    print(model.__class__.__name__)\n",
    "    get_test_metrics(model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
